# 感知机学习算法

感知机学习问题转化为求解损失函数的最优化问题，最优化的方法就是随机梯度下降法。

### 1. 学习算法的原始形式

给定一个训练数据集$$T=\{(x_1,y_1),(x_2,y_2),...,(x_n,y_n)\}$$，其中，$$x_i\in X= R^n$$，$$y_i\in Y=\lbrace+1,-1\rbrace$$，$$i=1,2,...,n$$，求参数$$w$$，$$b$$，使得其为以下损失函数极小化的解：


$$
\min_{w,b}L(w,b)=-\displaystyle\sum_{x_i\in M}y_i(w\cdot x_i+b)
$$


其中$$M$$为**误分类点**的集合。

假设误分类点集合$$M$$是固定的，那么损失函数$$L(w,b)$$的梯度由


$$
\nabla_w L(w,b)=-\displaystyle\sum_{x_i\in M}y_i x_i
$$



$$
\nabla_b L(w,b)=-\displaystyle\sum_{x_i\in M}y_i
$$


给出。

#### 1.1 **随机梯度下降算法**：

**输入：**训练数据集$$T=\{(x_1,y_1),(x_2,y_2),...,(x_n,y_n)\}$$，其中，$$x_i\in X= R^n$$，$$y_i\in Y=\lbrace+1,-1\rbrace$$，$$i=1,2,...,n$$，学习率为$$    \eta(0<\eta\leqslant1)$$

**输出：**$$w,b$$：感知机模型$$f(x)=sign(w\cdot x+b)$$

1. 选取初始值$$w_0,b_0$$
2. 在训练集中选取数据$$(x_i,y_i)$$
3. 如果$$y_i(w\cdot x_i+b)\leqslant0$$，则$$w \gets w+\eta y_i x_i$$，$$b    \gets b+\eta y_i$$
4. 转至步骤\(2\)，直至训练集里面的每个点都不是误分类点，这个过程中训练集中的点可能会被重复的选中并计算。

#### 1.2 **直观的解释**

当出现误分类点时，则调整$$w,b$$，更正超平面的方向，使其稍微转向正确的方向。

#### ![](/assets/PLA.PNG)1.3 算法的收敛性

可以证明，对于**线性可分**的数据集，感知机学习算法经过有限次迭代可以得到一个将训练数据集完全正确划分的分离超平面及感知机模型。

![](/assets/PLA3.PNG)



> 参考：林轩田，机器学习基石



