### 梯度下降法

梯度下降法（Gradient descent）或最速下降法（steepest descent）是求解无约束最优化问题的一种常用方法。

假设$$f(x)$$是$$R^n$$上具有一阶连续偏导数的函数。要求解的无约束最优化问题是：


$$
	\displaystyle\min_{x\in R^n}	 f(x)
$$
$$x^*$$表示目标函数的极小值点。

梯度下降是一种迭代算法。选取适当的初始值$$x^{(0)}$$，不断迭代，更新$$x$$的值，进行目标函数的极小化，直到收敛。

梯度的相反方向是使得函数下降最快的方向，因此在迭代的每一步，以负梯度的方向更新$$x$$的值，从而达到减少函数值的目的。

由于$$f(x)$$具有一阶连续偏导数，若第$$k$$次迭代的值为$$x^{(k)}$$，则可将$$f(x)$$在$$x^{(k)}$$附件进行一阶泰勒展开：


$$
f(x)=f(x^{(k)})+g^T_k(x-x^{(k)})
$$
这里$$g_k=g(x^{(k)}=\nabla f(x^{(k)})$$为$$f(x)$$在$$x^{(k)}$$的梯度。

这里求出第$$k+1$$次迭代值$$x^{(k+1)}$$：
$$
x^{(k+1)}\gets x^{(k)}+\lambda_k p_k
$$
其中$$p_k$$是搜索方向，取负梯度方向$$p_k=-\nabla f(x^{(k)})$$，$$\lambda_k>0$$是步长。

