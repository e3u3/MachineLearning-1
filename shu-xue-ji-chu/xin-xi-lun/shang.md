### 熵

在信息论中**熵**（entropy）是接收的每条消息中包含的信息的平均量，又被称为**信息熵**、**信源熵**、**平均自信息量**。

熵也可以理解为不确定性的量度，因为越随机的信源的熵越大。这里的想法是，比较不可能发生的事情，当它发生了，会提供更多的信息。

设$$X$$是一个取有限个值的离散随机变量，其概率分布为：
$$
P(X=x_i)=p_i, \  i=1,2,..,n
$$
则随机变量$$X$$的熵定义为：
$$
H(X)=\displaystyle-\sum_{i=1}^np_ilogp_i
$$
若$$p_i=0$$，则定义$$0log0=0$$。通常对数以2为底或以$$e$$为底，这时熵的单位分别作为比特（bit）或奈特（nat）。

由定义可知，熵依赖于$$X$$的分布，而与$$X$$的取值无关，所以可以将$$X$$的熵记作$$H(p)$$，即
$$
H(p)=\displaystyle-\sum_{i=1}^np_ilogp_i
$$
熵越大，随机变量的不确定性越大。

