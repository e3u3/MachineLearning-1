特征选择在于选取对训练集有分类能力的特征，这样可以提高决策树学习的效率。

通常特征选择的准则是信息增益或信息增益比。

### 信息增益

信息增益（information gain）表示得知特征$$X$$的信息而使得类$$Y$$的信息不确定性减少称。

特征$$A$$对训练数据集$$D$$的信息增益$$g(D,A)$$，定义为集合$$D$$的经验熵$$H(D)$$与特征$$A$$在给定条件下$$D$$的经验条件熵$$H(D|A)$$之差，即


$$
g(D,A)=H(D)-H(D|A)
$$


一般地，熵$$H(X)$$与条件熵$$H(Y|X)$$之差称为互信息\(mutual information\)。

关于[熵](/shu-xue-ji-chu/xin-xi-lun/shang.md)、[[条件熵](/shu-xue-ji-chu/xin-xi-lun/shang.md)](/shu-xue-ji-chu/xin-xi-lun/tiao-jian-shang.md)、[互信息](/shu-xue-ji-chu/xin-xi-lun/hu-xin-xi.md)参考链接。关于信息增益和互信息之间的差别，参考[https://www.zhihu.com/question/39436574](https://www.zhihu.com/question/39436574。)。

在给定训练数据集$$D$$和特征$$A$$，经验熵$$H(D)$$表示对数据集$$D$$进行分类的不确定性。而经验条件熵$$H(D|A)$$表示在特征$$A$$给定的条件下对数据集$$D$$进行分类的不确定性，那么它们的差就表示由于特征$$A$$而使得对数据集$$D$$的分类的不确定性减少的程度。信息增益大的特征具有更强的分类能力。

#### 信息增益的算法

假设训练数据集为$$D$$，$$|D|$$表示样本容量，即样本个数。设有$$K$$个类$$C_k$$，$$k=1,2,...,K$$，$$|C_k|$$为属于类$$C_k$$的样本个数，$$\displaystyle\sum_{k=1}^K|C_k|=|D|$$。

设特征$$A$$有$$n$$个不同的取值$${a_1,a_2,...,a_n}$$，根据特征$$A$$的取值将$$D$$划分为$$n$$个子集$$D_1,D_2,...,D_n$$，$$|D_i|$$为$$D_i$$的样本个数，$$\displaystyle\sum_{i=1}^n|D_k|=|D|$$。记子集$$D_i$$中属于类$$C_k$$的样本集合为$$D_{ik}$$，$$|D_{ik}|$$为$$D_{ik}$$的样本个数。

**算法：**

输入：训练数据集$$D$$和特征$$A$$

输出：特征$$A$$对训练数据集$$D$$的信息增益$$g(D,A)$$

1）计算数据集$$D$$的经验熵$$H(D)$$

$$H(D)=-\displaystyle\sum_{i=1}^n    \dfrac{|C_k|}{|D|}$$

示例：

贷款申请样本数据表：

| ID | 年龄 | 有工作 | 有自己的房子 | 信贷情况 | 类别 |
| :--- | :--- | :--- | :--- | :--- | :--- |
| 1 | 青年 | 否 | 否 | 一般 | 否 |
| 2 | 青年 | 否 | 否 | 好 | 否 |
| 3 | 青年 | 是 | 否 | 好 | 是 |
| 4 | 青年 | 是 | 是 | 一般 | 是 |
| 5 | 青年 | 否 | 否 | 一般 | 否 |
| 6 | 中年 | 否 | 否 | 一般 | 否 |
| 7 | 中年 | 否 | 否 | 好 | 否 |
| 8 | 中年 | 是 | 是 | 好 | 否 |
| 9 | 中年 | 否 | 是 | 非常好 | 是 |
| 10 | 中年 | 否 | 是 | 非常好 | 是 |
| 11 | 老年 | 否 | 是 | 非常好 | 是 |
| 12 | 老年 | 否 | 是 | 好 | 是 |
| 13 | 老年 | 是 | 否 | 好 | 是 |
| 14 | 老年 | 是 | 否 | 非常好 | 是 |
| 15 | 老年 | 否 | 否 | 一般 | 否 |



