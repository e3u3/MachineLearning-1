### 神经元模型

神经网络是由具有适应性的简单单元组成的广泛并行互连的网络，它的组织能够模拟生物神经系统对真实世界物体所作出的交互反应。

神经网络中最基本的成分是神经元（neuron）模型，即上述定义中的“简单单元”。在生物神经网络中，每个神经元与其他神经元相连，当它“兴奋”时，就会向相连的神经元发送化学物质，从而改变这些神经元内的电位；如果某神经元的电位超过一个“阈值”（threshold），那么它就会被激活，即“兴奋”起来，向其他神经元发送化学物质。

1943年，McMulloch 和 Pitts 将上述情形抽象为下图所示的简单模型，这就是一直沿用至今的“M-P神经元模型”。

![](/assets/mp-neuron.png)

> pic source:[https://www.jianshu.com/p/c46b6d890790](https://www.jianshu.com/p/c46b6d890790)

在这个模型中，神经元接收到来自$$n$$个其他神经元传递过来的输入信号，这些输入信号通过带权重的连接（connection）进行传递，神经元接收到总输入将与神经元的阈值进行比较，然后通过“激活函数”（activation function）处理以产生神经元的输出。

理想中的激活函数是如图a的阶跃函数，它将输入值映射为输出值为“0”或“1”，“1”对应于神经元兴奋，“0”对应于神经元抑制。但是阶跃函数具有不连续、不光滑等不太好的性质，因此实际常用$$Sigmoid$$函数作为激活函数。

![](/assets/activation-function.png)

> pic source:[https://www.jianshu.com/p/c46b6d890790](https://www.jianshu.com/p/c46b6d890790)

### 感知机

感知机（Perceptron）由两层神经元组成，输入层接收外界输入的信号后传递给输出层，输出层是M-P神经元，也称为“阈值逻辑单元”（threshold logic unit）。

![](/assets/perceptron.png)

感知机能容易地实现逻辑与、或、非运算。$$y=f(\displaystyle\sum_{i=1}^nw_ix_i-\theta)$$，假如$$f$$是上面的阶跃函数，则

* 与运算

令$$w_1=w_2=1$$，$$\theta = 2$$，则$$y=f(1 \cdot  x_1 + 1\cdot x_2 -2)$$，仅在$$x_1=x_2=1$$时，$$y=1$$，其他时候$$y=0$$。

* 或运算

令$$w_1=w_2=1$$，$$\theta = 0.5$$，则 $$y=f(1 \cdot x_1 + 1 \cdot x_2 -0.5)$$，当$$x_1 =1 $$或$$x_2=1$$的时候$$y=1$$。

* 非运算

令$$w_1 = -0.6$$，$$w_2 = 0$$，$$\theta = -0.5$$，则$$y=f(-0.6 \cdot x_1 + 0 \cdot x_2 +0.5)$$，当$$x_1=1$$时$$y=0$$；当$$x_2=0$$时，$$y=1$$。

实际上，我们能用感知机网络来计算任何逻辑功能，原因是上面的与或非是通用运算，那样我们可以在多个通用的运算上构建出任何运算。下面是构建一个异或运算。

| A | B | Y1=NOT And\(A,B\) | Y2=OR\(A,B\) | Y=XOR\(A,B\)=AND\(Y1,Y2\) |
| :--- | :--- | :--- | :--- | :--- |
| 0 | 0 | 1 | 0 | 0 |
| 0 | 1 | 1 | 1 | 1 |
| 1 | 0 | 1 | 1 | 1 |
| 1 | 1 | 0 | 1 | 0 |

![](/assets/peception-xor.PNG)

多层网络可以完成对更复杂函数的模拟。

![](/assets/peception-xo2r.PNG)

### S 型神经元

上面的感知机中一个权重或偏置的微小改动有时候会引起那个感知机的输出完全翻转，比如0变到1，那样的翻转可能接下来引起其余网络的行为以极其复杂的方式改变。因此在调整网络的参数的时候，有些输出被正确分类，其他的行为很可能以一些很难控制的方式被完全改变。

我们引入一种称之为S型神经元来克服这个问题。 S型神经元和感知机类似，但是被修改的权重和偏置只会引起输出的微小变化，这对让神经元网络学习起来很关键。

正如一个感知机，S型神经元有多个输入，$$x_1,x_2,...x_n$$。对每个输入有权重，$$w_1,w_2,...,w_n$$和一个总的偏置$$b$$。但是它的输出不是0或1，它现在是$$\sigma( w \cdot x +b)$$，这里$$\sigma$$被称之为S型函数。定义为：


$$
\sigma(z) = \frac{1}{1+e^{-z}}
$$


也就是：


$$
\sigma(z) = \frac{1}{1+\mathrm{exp}(-\displaystyle\sum_{i=1}^nw_ix_i-b)}
$$


$$\sigma$$的平滑特性意味着权重和偏置的微小变化，即$$\Delta w_i$$和$$\Delta b$$，会从神经元产生一个微小的输出变化$$\Delta \mathrm{output}$$，实际上，微积分告诉我们输出变化可以很好的近似表示为（一阶泰勒展开）：


$$
\Delta \mathrm{output} \approx \displaystyle\sum_{i=1}\frac{\partial \mathrm{output}}{\partial w_i}\Delta w_i + \frac{\partial \mathrm{output}}{\partial b}\Delta b
$$


上面的式子表明，$$\Delta \mathrm{output}$$是一个反映权重和偏置变化（$$\Delta w_i$$和$$\Delta b$$）的线性函数，这一线性使得选择权重和偏置的微小变化来达到输出的微小变化变得很容易。

