### 反向传播算法

我们希望有个算法，能够让我们找到权重和偏置，以至于神经网络的输出$$y(x)$$能够拟合所有的训练输入$$x$$。为了量化我们如何实现这个目标，我们定义一个代价函数：


$$
C(w,b)=\frac{1}{2n}\displaystyle\sum_{x}||y(x)-a^L(x)||^2
$$


这里$$w$$表示所有网络中权重的集合，$$b$$是所有的偏置，$$n$$是训练输入数据的个数，$$L$$表示网络的层数，$$a^L=a^L(x)$$是表示当输入为$$x$$时的网络输出的激活值向量，求和则是在总的训练输出$$x $$上进行的。符号$$||v||$$是指向量$$v$$的模。我们把$$C$$称为**二次代价函数**；有时也别称为**均方误差**或者**MSE**。

代价函数$$C(w,b)$$是非负的，因为求和公式中的每一项都是非负的。此外，当对于所有的训练输入$$x $$，$$y(x)$$接近于输出$$a$$时，代价函数的值相当小，即$$C(w,b)\approx0$$。

反向传播算法给出了一个计算代价函数梯度的的方法。

1. 输入$$x$$：为输入层设置对应的激活值$$a^l$$
2. 前向传播：对每个$$l=2,3,...,L$$计算相应的的$$z^l=w^l\cdot a^{(l-1)} + b^l$$和$$a^l = \sigma(z^l)$$
3. 输出层误差$$\delta^L$$：计算向量$$\delta^L=\nabla_aC \bigodot\sigma'(z^L)$$
4. 反向误差传播：对每个$$l= L-1, L-2, ..., 2$$，计算$$\delta^l=(({w^{(l+1)}}^T)\delta^{(l+1)})\bigodot\sigma'(z^L)$$
5. 输出：代价函数的梯度由$$\frac{\partial C}{\partial W^l_{jk}}=a^{l-1}_k\delta^l_j$$和$$\frac{\partial C}{\partial b^l_{j}}=\delta^l_j$$得出。



