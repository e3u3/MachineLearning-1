### 朴素贝叶斯法

朴素贝叶斯（native Bayes）法是基于贝叶斯定理与特征条件独立假设的分类方法。

对于给定的训练集，首先基于特征条件独立假设学习输入/输出的联合概率分布；然后基于此模型，对于给定的输入$$x $$，利用贝叶斯定理求出后验概率最大的输出$$y$$。

#### 基本方法

假设输入空间$$\mathcal{X}\subseteq R^n$$为$$n$$维向量的集合，输出空间为类标记集合$$\mathcal{Y}=\{c_1, c_2,...,c_K\}$$。输入为特征向量$$x\in \mathcal{X}$$，输出为类标记$$y\in \mathcal{Y}$$。$$X$$是定义在输入空间$$\mathcal{X}$$上的随机向量，$$Y$$是定义在输出空间$$\mathcal{Y}$$上的随机变量。$$P(X,Y)$$是$$X$$和$$Y$$的联合概率分布。训练数据集$$T=\{(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),...,(x^{(m)},y^{(m)})\}$$是由$$P(X,Y)$$独立同分布产生的，其中每个$$x=(x_1, x_2,...,x_n)$$是$$n$$维向量。

朴素贝叶斯法通过对给定的输入$$x$$，通过学习到的模型计算**后验概率分布**$$P(Y=c_k|X=x)$$，然后将后验概率最大的类作为$$x $$的输出。计算后验概率：


$$
P(Y=c_k|X=x)=\dfrac{P(Y=c_k, X=x)}{P(X=x)}=\dfrac{P(X=x|Y=c_k)P(Y=c_k)}{\displaystyle\sum_{k=1}^KP(X=x|Y=c_k)P(Y=c_k)}
$$


其中


$$
P(Y=c_k), \ k=1,2,...,K
$$


是先验概率分布。


$$
P(X=x|Y=c_k)=P(X_1=x_1, X_2=x_2,...,X_n=x_n|Y=c_k), \ k=1,2,...,K
$$


是条件概率分布（似然函数）。假定条件概率分布中的每个特征是条件独立的，则
$$
P(X=x|Y=c_k)=\prod_{j=1}^n P(X_j=x_j|Y=c_k)
$$




