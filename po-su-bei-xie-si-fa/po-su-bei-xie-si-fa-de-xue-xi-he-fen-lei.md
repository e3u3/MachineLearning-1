### 朴素贝叶斯法

朴素贝叶斯（native Bayes）法是基于贝叶斯定理与特征条件独立假设的分类方法。

对于给定的训练集，首先基于特征条件独立假设学习输入/输出的联合概率分布；然后基于此模型，对于给定的输入$$x $$，利用贝叶斯定理求出后验概率最大的输出$$y$$。

#### 基本方法

假设输入空间$$\mathcal{X}\subseteq R^n$$为$$n$$维向量的集合，输出空间为类标记集合$$\mathcal{Y}=\{c_1, c_2,...,c_K\}$$。输入为特征向量$$x\in \mathcal{X}$$，输出为类标记$$y\in \mathcal{Y}$$。$$X$$是定义在输入空间$$\mathcal{X}$$上的随机向量，$$Y$$是定义在输出空间$$\mathcal{Y}$$上的随机变量。$$P(X,Y)$$是$$X$$和$$Y$$的联合概率分布。训练数据集$$T=\{(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),...,(x^{(m)},y^{(m)})\}$$是由$$P(X,Y)$$独立同分布产生的，其中每个$$x=(x_1, x_2,...,x_n)$$是$$n$$维向量。

朴素贝叶斯法通过对给定的输入$$x$$，通过学习到的模型计算**后验概率分布**$$P(Y=c_k|X=x)$$，然后将**后验概率最大**的类作为$$x $$的输出。计算后验概率：


$$
P(Y=c_k|X=x)=\dfrac{P(Y=c_k, X=x)}{P(X=x)}=\dfrac{P(X=x|Y=c_k)P(Y=c_k)}{\displaystyle\sum_{k=1}^KP(X=x|Y=c_k)P(Y=c_k)}
$$


其中$$k=1,2,...,K$$，可以看到分母对于所有的类标记$$c_k$$都是相同的，则可以得到输出


$$
y=\arg \max_{c_k}P(X=x|Y=c_k)P(Y=c_k)
$$


其中


$$
P(Y=c_k), \ k=1,2,...,K
$$


是**先验概率**分布。


$$
P(X=x|Y=c_k)=P(X_1=x_1, X_2=x_2,...,X_n=x_n|Y=c_k), \ k=1,2,...,K
$$


是条件概率分布（**似然函数**）。假定条件概率分布中的每个特征是**条件独立**的，则


$$
P(X=x|Y=c_k)=\prod_{j=1}^n P(X_j=x_j|Y=c_k)
$$


这一假设使得朴素贝叶斯法变得简单，但是会牺牲一定的分类准确率。

于是代入，可以得到：


$$
y=f(x)=\arg \max_{c_k}\prod_{j=1}^n P(X_j=x_j|Y=c_k)P(Y=c_k)
$$


朴素贝叶斯法属于**生成模型**（模型给定了输入$$X$$产生输出$$Y$$的生成关系，区别于**判别模型**）

#### 模型的原理

首先，定义0-1损失函数：
$$
L(Y,f(X)) = \begin{cases}
   1, &\text{if } Y\not=f(X)  \\
   0, &\text{if } Y=f(X)
\end{cases}
$$
其中$$f(X)$$是分类决策函数的**预测值**，$$Y$$是**真实值**。这时，损失函数的期望是
$$
R_{exp}(f)=E[L(Y,f(X))]
$$
对于输入$$X=x$$，计算$$X=x$$条件下的期望损失函数，即条件期望
$$
E[L(Y,f(X=x))|X=x]=\displaystyle\sum_{k=1}^KL(c_k, f(X=x))P(c_k|X=x)
$$
当$$Y=f(X=x)$$时，$$L(c_k, f(X=x))=0$$，则：
$$
\displaystyle\sum_{k=1}^KL(c_k, f(X=x))P(c_k|X=x)=\displaystyle\sum_{Y\not=f(X=x)}P(Y|X=x)
$$
则在$$X=x$$条件下，为了使得期望风险最小化，
$$
f(x)=\arg\max_{y\in \mathcal{Y}}\displaystyle\sum_{k=1}^KL(c_k, f(X=x))P(c_k|X=x)
$$


对于

